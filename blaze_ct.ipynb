{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Ge1crpnBsqf8FSZjKAVr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aradhana148/Medical-NER-RE/blob/master/blaze_ct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D9cCVbpck92",
        "outputId": "f7f48c91-ab00-4c54-d845-60bac946cbe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the HF_TOKEN from Colab secrets if it's needed for this model\n",
        "# If you've already set it globally, this might not be strictly necessary here, but it's good practice.\n",
        "# Ensure your 'HF_TOKEN' secret is set in Colab.\n",
        "if \"HF_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"blaze999/Medical-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"blaze999/Medical-NER\")\n"
      ],
      "metadata": {
        "id": "hkexDVxndazh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "from typing import List, Tuple, Dict, Any, Optional"
      ],
      "metadata": {
        "id": "I9nhAcyadcz2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/ct_data.csv\")\n",
        "\n",
        "clean = (\n",
        "    df.iloc[:, 0]\n",
        "      .astype(str)\n",
        "      .str.replace(r'^FINDINGS:\\s*', '', regex=True)\n",
        "      .str.replace(r\"\\s+\", \" \", regex=True)\n",
        "      .str.strip()\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "concepts = df.iloc[:, 1].reset_index(drop=True)\n",
        "print(concepts[0:3])\n",
        "print(clean[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Wu9iKh1dftn",
        "outputId": "e70c84e9-65d9-42f8-9506-2014e1753acd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    ['patent air passages', 'nodular consolidation...\n",
            "1    ['patent trachea', 'patent bronchi', 'peribron...\n",
            "2    ['patent trachea', 'patent bronchi', 'no occlu...\n",
            "Name: concepts, dtype: object\n",
            "0    The air passages of the trachea, both main bro...\n",
            "1    The trachea and both main bronchi are centrall...\n",
            "2    The trachea and the lumens of both main bronch...\n",
            "Name: report, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from numpy._core.defchararray import endswith\n",
        "\n",
        "# rows = []\n",
        "# for i in range(len(clean)):\n",
        "#   text = clean[i]\n",
        "\n",
        "#   # Tokenize the input text\n",
        "#   inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "#   # Get model predictions\n",
        "#   outputs = model(**inputs)\n",
        "\n",
        "#   # Process the output to get the predicted labels\n",
        "#   import torch\n",
        "#   predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "#   # Decode the predictions to human-readable tags\n",
        "#   # Assuming the model's config has id2label mapping\n",
        "#   id2label = model.config.id2label\n",
        "\n",
        "#   tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "#   predicted_labels = [id2label[p.item()] for p in predictions[0]]\n",
        "\n",
        "#   results = []\n",
        "#   for token, label in zip(tokens, predicted_labels):\n",
        "#     if token.startswith(\"##\"):\n",
        "#         results[-1][0] += token.replace(\"##\", \"\")\n",
        "#     else:\n",
        "#         results.append([token, label])\n",
        "\n",
        "#   # Filter out special tokens like [CLS] and [SEP]\n",
        "#   final_results = []\n",
        "#   for token, label in results:\n",
        "#     if token not in ['[CLS]', '[SEP]', '[PAD]']:\n",
        "#         final_results.append([token, label])\n",
        "#   rows.append({\n",
        "#         \"report\": text,\n",
        "#         \"ner_output\": repr(final_results),        # store as string; my parser ast.literal_eval can read it\n",
        "#         \"concepts\": concepts[i]\n",
        "#     })\n",
        "\n",
        "# df_optA = pd.DataFrame(rows)\n",
        "# df_optA.to_csv(\"option_a.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "7zzZh-oPd00W"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _is_alphaish(w: str) -> bool:\n",
        "    # Simple check for now, can be expanded if needed.\n",
        "    # This function was missing and causing a NameError.\n",
        "    return w.isalpha()"
      ],
      "metadata": {
        "id": "e24p6QDYd5SP"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7ee788"
      },
      "source": [
        "# --------- Negation + uncertainty cues (generic, not medical) ----------\n",
        "NEG_CUES = {\"no\", \"without\", \"absent\", \"denies\", \"deny\", \"none\"}   # IMPORTANT: removed \"not\"\n",
        "CONTRAST = {\"but\", \"however\", \"though\", \"although\"}\n",
        "UNCERTAIN_CUES = {\"likely\", \"probably\", \"possibly\"}\n",
        "\n",
        "SENT_END = {\".\", \"!\", \"?\"}\n",
        "HARD_BREAK = {\".\", \";\"}\n",
        "\n",
        "STOPWORDS = {\n",
        "    \"the\",\"a\",\"an\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"of\",\"to\",\"in\",\"on\",\"for\",\"with\",\n",
        "    \"and\",\"or\",\"as\",\"by\",\"at\",\"from\",\"that\",\"this\",\"these\",\"those\",\"it\",\"its\",\"there\",\"appears\",\n",
        "    \"appear\",\"seen\",\"noted\",\"present\",\"provided\",\"demonstrate\",\"demonstrates\",\"detected\",\"given\",\n",
        "    \"which\",\"who\",\"whom\",\"whose\",\"into\",\"over\",\"under\",\"below\",\"above\",\"within\",\"without\"\n",
        "}\n",
        "\n",
        "NON_GLUE_WORDS = {\n",
        "    \"left\",\"right\",\"upper\",\"lower\",\"middle\",\"bilateral\",\"bilaterally\",\n",
        "    \"small\",\"large\",\"moderate\",\"mild\",\"severe\",\"acute\",\"chronic\",\"old\",\"new\",\n",
        "    \"free\",\"air\",\"below\",\"above\",\"midline\",\"frontal\",\"lateral\",\"views\",\"view\",\n",
        "    \"worse\",\"better\",\"stable\",\"unchanged\",\"likely\",\"probably\",\"possibly\"\n",
        "}\n",
        "\n",
        "PUNCT = {\".\", \",\", \";\", \":\", \"!\", \"?\", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"/\", \"\\\\\"}\n",
        "\n",
        "# ----------------------------\n",
        "# Parsing NER output\n",
        "# ----------------------------\n",
        "def _as_obj(x):\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    if isinstance(x, str):\n",
        "        return ast.literal_eval(x)\n",
        "    raise TypeError(f\"Unsupported ner_output type: {type(x)}\")\n",
        "\n",
        "def _pair_from_item(item) -> Tuple[str, str]:\n",
        "    # Your new format: [token, label]\n",
        "    if isinstance(item, (list, tuple)) and len(item) == 2 and isinstance(item[0], str) and isinstance(item[1], str):\n",
        "        return item[0], item[1]\n",
        "\n",
        "    # legacy formats (keep)\n",
        "    if isinstance(item, dict):\n",
        "        k, v = next(iter(item.items()))\n",
        "        if isinstance(k, str) and (k == \"O\" or re.match(r\"^(B|I)-\", k)):\n",
        "            return v, k\n",
        "        return k, v\n",
        "\n",
        "    if isinstance(item, (set, tuple, list)) and len(item) == 2:\n",
        "        a, b = list(item)\n",
        "        if isinstance(a, str) and (a == \"O\" or re.match(r\"^(B|I)-\", a)):\n",
        "            return b, a\n",
        "        if isinstance(b, str) and (b == \"O\" or re.match(r\"^(B|I)-\", b)):\n",
        "            return a, b\n",
        "        return a, b\n",
        "\n",
        "    raise ValueError(f\"Unsupported item format: {item}\")\n",
        "\n",
        "def parse_ner_output(ner_output) -> List[Tuple[str, str]]:\n",
        "    obj = _as_obj(ner_output)\n",
        "    return [_pair_from_item(it) for it in obj]\n",
        "\n",
        "# ----------------------------\n",
        "# Token rebuild (NO word splitting, NO punctuation glue)\n",
        "# Works for WordPiece (##) and SentencePiece (▁)\n",
        "# ----------------------------\n",
        "def rebuild_tokens(tokens_tags: List[Tuple[str, str]]) -> Tuple[List[str], List[str]]:\n",
        "    toks: List[str] = []\n",
        "    tags: List[str] = []\n",
        "\n",
        "    for tok, tag in tokens_tags:\n",
        "        tok = \"\" if tok is None else tok\n",
        "\n",
        "        # If punctuation token, keep separate always\n",
        "        if tok in PUNCT:\n",
        "            toks.append(tok)\n",
        "            tags.append(tag)\n",
        "            continue\n",
        "\n",
        "        # WordPiece continuation\n",
        "        if tok.startswith(\"##\"):\n",
        "            piece = tok[2:]\n",
        "            if toks and toks[-1] not in PUNCT:\n",
        "                toks[-1] += piece\n",
        "            else:\n",
        "                toks.append(piece)\n",
        "                tags.append(tag)\n",
        "            continue\n",
        "\n",
        "        # SentencePiece: ▁ indicates new word boundary\n",
        "        if tok.startswith(\"▁\"):\n",
        "            tok = tok[1:]\n",
        "\n",
        "        # Normal new token (do NOT append to previous)\n",
        "        toks.append(tok)\n",
        "        tags.append(tag)\n",
        "\n",
        "    # normalize empties\n",
        "    toks = [t for t in toks if t != \"\"]\n",
        "    tags = tags[:len(toks)]\n",
        "    return toks, tags\n",
        "\n",
        "# ----------------------------\n",
        "# BIO span extraction\n",
        "# ----------------------------\n",
        "def _join_span_tokens(span_tokens):\n",
        "    out: List[str] = []\n",
        "    for t in span_tokens:\n",
        "        if not out:\n",
        "            out.append(t)\n",
        "        else:\n",
        "            # Concatenate if 't' is a hyphen or the last token in 'out' is a hyphen\n",
        "            if t == '-' or out[-1] == '-':\n",
        "                out[-1] = out[-1] + t\n",
        "            else:\n",
        "                out.append(t)\n",
        "    return \" \".join(out)\n",
        "\n",
        "def extract_spans(toks, tags):\n",
        "    spans = []\n",
        "    i = 0\n",
        "    while i < len(tags):\n",
        "        tag = tags[i]\n",
        "        if tag == \"O\" or not isinstance(tag, str):\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        m = re.match(r\"^(B|I)-(.+)$\", tag)\n",
        "        if not m:\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        bio, etype = m.group(1), m.group(2)\n",
        "        if bio == \"I\":\n",
        "            bio = \"B\"\n",
        "\n",
        "        start = i\n",
        "        j = i + 1\n",
        "        while j < len(tags):\n",
        "            m2 = re.match(r\"^I-(.+)$\", tags[j] if isinstance(tags[j], str) else \"\")\n",
        "            if m2 and m2.group(1) == etype:\n",
        "                j += 1\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        # join tokens with smart glue (instead of always \" \".join)\n",
        "        span_tokens = [toks[k] for k in range(start, j) if toks[k].strip() != \"\" and toks[k] not in PUNCT]\n",
        "        text = _join_span_tokens(span_tokens)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "        spans.append({\"type\": etype, \"text\": text, \"start\": start, \"end\": j})\n",
        "        i = j\n",
        "\n",
        "    return spans\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Sentence segmentation\n",
        "# ----------------------------\n",
        "def split_into_segments(toks: List[str]) -> List[Tuple[int, int]]:\n",
        "    segs = []\n",
        "    s = 0\n",
        "    for i, t in enumerate(toks):\n",
        "        if t in SENT_END:\n",
        "            segs.append((s, i + 1))\n",
        "            s = i + 1\n",
        "    if s < len(toks):\n",
        "        segs.append((s, len(toks)))\n",
        "    return segs\n",
        "\n",
        "# ----------------------------\n",
        "# Negation scope per sentence\n",
        "# ----------------------------\n",
        "def compute_negated_span_ids(toks, spans, seg_start, seg_end) -> set:\n",
        "    cue_positions = []\n",
        "    for i in range(seg_start, seg_end):\n",
        "        w = toks[i].lower()\n",
        "        if w in NEG_CUES:\n",
        "            cue_positions.append(i)\n",
        "\n",
        "    negated = set()\n",
        "    for cue in cue_positions:\n",
        "        scope_end = seg_end\n",
        "        for j in range(cue + 1, seg_end):\n",
        "            w = toks[j].lower()\n",
        "            if toks[j] in HARD_BREAK or w in CONTRAST:\n",
        "                scope_end = j\n",
        "                break\n",
        "\n",
        "        for si, sp in enumerate(spans):\n",
        "            if cue < sp[\"start\"] < scope_end:\n",
        "                negated.add(si)\n",
        "\n",
        "    return negated\n",
        "\n",
        "# ----------------------------\n",
        "# Normalization helpers (generic)\n",
        "# ----------------------------\n",
        "def norm_phrase(s: str) -> str:\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    s = s.strip(\" ,;:.\")\n",
        "    s = s.lower()\n",
        "\n",
        "    s = re.sub(r\"\\bwithin normal limits\\b\", \"normal\", s)\n",
        "    s = re.sub(r\"\\bunremarkable\\b\", \"normal\", s)\n",
        "    s = re.sub(r\"\\btop[- ]normal\\b\", \"normal\", s)\n",
        "    s = re.sub(r\"\\bno evidence of\\b\", \"no\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def dedup_preserve_order(items: List[str]) -> List[str]:\n",
        "    seen = set()\n",
        "    out = []\n",
        "    for x in items:\n",
        "        x = norm_phrase(x)\n",
        "        if x and x not in seen:\n",
        "            seen.add(x)\n",
        "            out.append(x)\n",
        "    return out\n",
        "\n",
        "def token_window(toks, a, b):\n",
        "    return [toks[i] for i in range(a, b) if 0 <= i < len(toks)]\n",
        "\n",
        "def has_uncertainty(toks, a, b) -> bool:\n",
        "    ws = [w.lower() for w in token_window(toks, a, b)]\n",
        "    return any(w in UNCERTAIN_CUES for w in ws)\n",
        "\n",
        "# ----------------------------\n",
        "# Concept building (no medical vocabulary)\n",
        "# ----------------------------\n",
        "def spans_in_range(spans, a, b):\n",
        "    return [sp for sp in spans if a <= sp[\"start\"] < b]\n",
        "\n",
        "def collect_left_mods(seg_spans, head_span, mod_types, max_gap=6):\n",
        "    mods = [sp for sp in seg_spans\n",
        "            if sp[\"type\"] in mod_types and sp[\"end\"] <= head_span[\"start\"]\n",
        "            and (head_span[\"start\"] - sp[\"end\"]) <= max_gap]\n",
        "    mods.sort(key=lambda x: x[\"start\"])\n",
        "    return [m[\"text\"] for m in mods if m[\"text\"]]\n",
        "\n",
        "def build_concepts_from_segment(toks, spans, seg, neg_ids):\n",
        "    seg_start, seg_end = seg\n",
        "    seg_spans = spans_in_range(spans, seg_start, seg_end)\n",
        "\n",
        "    VALUE_TYPES  = {\"LAB_VALUE\"}\n",
        "    MOD_TYPES    = {\"DETAILED_DESCRIPTION\", \"SEVERITY\"}\n",
        "    HEAD_TYPES   = {\"SIGN_SYMPTOM\", \"DISEASE_DISORDER\"}\n",
        "    STRUCT_TYPES = {\"BIOLOGICAL_STRUCTURE\"}\n",
        "    PROC_TYPES   = {\"DIAGNOSTIC_PROCEDURE\", \"THERAPEUTIC_PROCEDURE\"}\n",
        "\n",
        "    concepts = []\n",
        "\n",
        "    # (A) Merge adjacent procedure spans with connectors (e.g., ‘X and Y …’)\n",
        "    proc = [sp for sp in seg_spans if sp[\"type\"] in PROC_TYPES and sp[\"text\"]]\n",
        "    proc.sort(key=lambda x: x[\"start\"])\n",
        "    i = 0\n",
        "    while i < len(proc):\n",
        "        cur = proc[i]\n",
        "        parts = [cur[\"text\"]]\n",
        "        end = cur[\"end\"]\n",
        "        j = i + 1\n",
        "        while j < len(proc):\n",
        "            # allow small gap with connector tokens like \"and\", \"of\", \"the\"\n",
        "            gap_tokens = [t.lower() for t in token_window(toks, end, proc[j][\"start\"])]\n",
        "            gap_tokens = [t for t in gap_tokens if t not in PUNCT and t != \"\"]\n",
        "            if len(gap_tokens) <= 3 and all(t in {\"and\",\"of\",\"the\"} for t in gap_tokens):\n",
        "                parts.append(proc[j][\"text\"])\n",
        "                end = proc[j][\"end\"]\n",
        "                j += 1\n",
        "            else:\n",
        "                break\n",
        "        concepts.append(\" \".join(parts))\n",
        "        i = j\n",
        "\n",
        "    # (B) Value + Structure: only if value is ‘normal-like’ or ‘clear-like’ (generic)\n",
        "    # We do it without hardcoding medical terms: we just constrain value to common adjectives.\n",
        "    NORMAL_LIKE = {\"normal\", \"clear\", \"stable\", \"intact\"}\n",
        "    values = [sp for sp in seg_spans if sp[\"type\"] in VALUE_TYPES and norm_phrase(sp[\"text\"]) in NORMAL_LIKE]\n",
        "    structs = [sp for sp in seg_spans if sp[\"type\"] in STRUCT_TYPES and sp[\"text\"]]\n",
        "\n",
        "    for v in values:\n",
        "        vtxt = norm_phrase(v[\"text\"])\n",
        "        for st in structs:\n",
        "            # same clause-ish: close and not across sentence end\n",
        "            if abs(st[\"start\"] - v[\"start\"]) <= 10:\n",
        "                # optional suffix word if the next token is NOT stopword/punct\n",
        "                suffix = \"\"\n",
        "                after = st[\"end\"]\n",
        "                if after < seg_end:\n",
        "                    w = toks[after].strip().lower()\n",
        "                    if w and w not in PUNCT and w not in STOPWORDS:\n",
        "                        suffix = \" \" + toks[after].strip()\n",
        "                concepts.append(f\"{vtxt} {st['text']}{suffix}\")\n",
        "\n",
        "    # (C) Head spans with left modifiers; attach a nearby structure if present\n",
        "    heads = [(si, sp) for si, sp in enumerate(spans) if sp in seg_spans and sp[\"type\"] in HEAD_TYPES and sp[\"text\"]]\n",
        "    for si, h in heads:\n",
        "        mods = collect_left_mods(seg_spans, h, MOD_TYPES, max_gap=6)\n",
        "        phrase = \" \".join([*mods, h[\"text\"]]).strip()\n",
        "\n",
        "        # If there is a nearby structure, attach it (helps ‘calcified <structure>’, etc.)\n",
        "        # Only do this when head is short (1–2 tokens) to avoid huge phrases.\n",
        "        if len(phrase.split()) <= 2:\n",
        "            # Prefer a structure immediately BEFORE the head (fixes: \"pleural abnormality\")\n",
        "            left_struct = None\n",
        "            for st in structs:\n",
        "                if 0 <= (h[\"start\"] - st[\"end\"]) <= 3:\n",
        "                    left_struct = st\n",
        "                    break\n",
        "            if left_struct is not None:\n",
        "                phrase = f\"{left_struct['text']} {phrase}\"\n",
        "            else:\n",
        "                # Otherwise attach structure immediately AFTER the head\n",
        "                right_struct = None\n",
        "                for st in structs:\n",
        "                    if 0 <= (st[\"start\"] - h[\"end\"]) <= 3:\n",
        "                        right_struct = st\n",
        "                        break\n",
        "                if right_struct is not None:\n",
        "                    phrase = f\"{phrase} {right_struct['text']}\"\n",
        "\n",
        "\n",
        "        # uncertainty cue in local window around head\n",
        "        if has_uncertainty(toks, max(seg_start, h[\"start\"] - 4), min(seg_end, h[\"end\"] + 4)):\n",
        "            phrase = \"likely \" + phrase\n",
        "\n",
        "        phrase = norm_phrase(phrase)\n",
        "\n",
        "        if si in neg_ids and not phrase.startswith(\"no \"):\n",
        "            phrase = \"no \" + phrase\n",
        "\n",
        "        concepts.append(phrase)\n",
        "\n",
        "    # (D) Generic ‘status post …’ ⇒ ‘status post surgery’ if any procedure span exists\n",
        "    seg_text = \" \".join([t.lower() for t in token_window(toks, seg_start, seg_end)])\n",
        "    if \"status post\" in seg_text and any(sp[\"type\"] == \"THERAPEUTIC_PROCEDURE\" for sp in seg_spans):\n",
        "        concepts.append(\"status post surgery\")\n",
        "\n",
        "    # (E) ‘difficult to assess <structure> due to <head>’ pattern (generic)\n",
        "    if \"difficult to assess\" in seg_text:\n",
        "        # pick first structure in this segment if exists\n",
        "        if structs:\n",
        "            target = structs[0][\"text\"]\n",
        "            # pick nearest head span after ‘due to’ if present, else nearest head\n",
        "            due_idx = None\n",
        "            for idx in range(seg_start, seg_end - 1):\n",
        "                if toks[idx].lower() == \"due\" and toks[idx+1].lower() == \"to\":\n",
        "                    due_idx = idx\n",
        "                    break\n",
        "            due_head = None\n",
        "            if due_idx is not None:\n",
        "                for _, h in heads:\n",
        "                    if h[\"start\"] > due_idx:\n",
        "                        due_head = h[\"text\"]\n",
        "                        break\n",
        "            if due_head:\n",
        "                concepts.append(f\"difficult to assess {target} due to {due_head}\")\n",
        "            else:\n",
        "                concepts.append(f\"difficult to assess {target}\")\n",
        "\n",
        "    return dedup_preserve_order(concepts)\n",
        "\n",
        "def postprocess(ner_output) -> List[str]:\n",
        "    tokens_tags = parse_ner_output(ner_output)\n",
        "    toks, tags = rebuild_tokens(tokens_tags)\n",
        "    spans = extract_spans(toks, tags)\n",
        "    segments = split_into_segments(toks)\n",
        "\n",
        "    all_concepts = []\n",
        "    for seg in segments:\n",
        "        neg_ids = compute_negated_span_ids(toks, spans, seg[0], seg[1])\n",
        "        all_concepts.extend(build_concepts_from_segment(toks, spans, seg, neg_ids))\n",
        "\n",
        "    return dedup_preserve_order(all_concepts)\n",
        "\n",
        "# CSV runner\n",
        "import pandas as pd\n",
        "def run_on_csv(path: str, ner_col: str = \"ner_output\") -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    preds = []\n",
        "    for x in df[ner_col].astype(str).tolist():\n",
        "        ner_obj = ast.literal_eval(x)\n",
        "        preds.append(postprocess(ner_obj))\n",
        "    out = df.copy()\n",
        "    out[\"pred_concepts\"] = preds\n",
        "    return out"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load Option-A CSV\n",
        "df = pd.read_csv(\"/content/option_a.csv\")   # change path if needed\n",
        "\n",
        "# Parse ner_output column (stored as string)\n",
        "df[\"ner_output_parsed\"] = df[\"ner_output\"].apply(ast.literal_eval)\n",
        "\n",
        "# Run post-processing\n",
        "df[\"pred_concepts\"] = df[\"ner_output_parsed\"].apply(postprocess)\n",
        "\n",
        "# Optional: parse ground-truth concepts if you want to print them\n",
        "def parse_gt(x):\n",
        "    if isinstance(x, list):\n",
        "        return x\n",
        "    if pd.isna(x):\n",
        "        return []\n",
        "    try:\n",
        "        return ast.literal_eval(x)\n",
        "    except Exception:\n",
        "        return [str(x)]\n",
        "\n",
        "df[\"gt_concepts\"] = df[\"concepts\"].apply(parse_gt)\n",
        "\n",
        "# Print a few examples\n",
        "for i in range(5):\n",
        "    print(\"REPORT:\")\n",
        "    print(df.loc[i, \"report\"])\n",
        "    print(\"\\nPREDICTED CONCEPTS:\")\n",
        "    print(df.loc[i, \"pred_concepts\"])\n",
        "    print(\"\\nGROUND TRUTH:\")\n",
        "    print(df.loc[i, \"gt_concepts\"])\n",
        "    print(\"=\" * 80)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvuDYjsrd8z3",
        "outputId": "80649936-d736-4516-9b65-5e6ee174f354"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPORT:\n",
            "The air passages of the trachea, both main bronchi, lobar, and segmental bronchi are patent. There is an irregularly circumscribed nodular consolidation area adjacent to the diaphragm in the basal segment of the lower lobe of the right lung. A Reverse Halo sign is observed and is suspicious for invasive fungal infection. No involvement is observed in other parenchymal areas. No pleural effusion is detected. Heart size is within normal limits. No pericardial effusion is detected. Calibration of mediastinal major vascular structures is normal. Evaluation of mediastinal structures is suboptimal due to the absence of contrast material. Lymph nodes with diameters of 12 mm and 13 mm are observed in the carina and subcarinal areas. A central venous catheter is present. In the supraclavicular fossa and axilla, no lymph nodes of pathological size and appearance are observed. No lytic or destructive lesions are detected in the bone structures. No abnormalities are detected in the upper abdominal sections included in the image.\n",
            "\n",
            "PREDICTED CONCEPTS:\n",
            "['segmental bronchi patent', 'irregularly circumscribed nodular consolidation area', 'reverse halo sign', 'invasive fungal infection', 'no involvement', 'no pleural effusion', 'heart size', 'no pericardial effusion', 'calibration', 'normal media stin al major vascular structures', 'evaluation', 'lymph nodes', 'central venous catheter', 'no a x illa lymph nodes', 'no lytic or lesions are', 'no are']\n",
            "\n",
            "GROUND TRUTH:\n",
            "['patent air passages', 'nodular consolidation', 'Reverse Halo sign suspicious for invasive fungal infection', 'no pleural effusion', 'normal heart size', 'no pericardial effusion', 'normal mediastinal vascular structures', 'lymph nodes of 12mm and 13mm in carina', 'central venous catheter', 'no pathological lymph nodes in supraclavicular fossa and axilla', 'normal bone structures', 'no abnormalities in upper abdominal sections']\n",
            "================================================================================\n",
            "REPORT:\n",
            "The trachea and both main bronchi are centrally positioned and patent. Segmental and subsegmental peribronchial thickening with prominent luminal narrowing is observed in both lungs. A mosaic attenuation pattern is noted in both lungs, likely secondary to small airway disease. Reticulonodular density increases and areas of paraseptal emphysema are present in both lung apices. Several subcentimeter nonspecific pulmonary nodules are observed in both lungs. No mass or active infiltration is detected in the lung parenchyma. Mediastinal main vascular structures and heart contour and size appear normal. No pericardial effusion or thickening is evident. Calcific atheroma plaques are observed in the aortic arch and its supraaortic branches. Mediastinal evaluation is limited on this non-contrast exam. Thoracic esophagus is normal in caliber without significant wall thickening. No enlarged lymph nodes are detected in prevascular, pre-paratracheal, subcarinal, or bilateral hilar-axillary regions. Bone structures within the field of view are normal. Vertebral body heights are preserved. Upper abdominal organs appear normal within the field of view. There are no liver lesions within the field of view. The bilateral adrenal glands appear normal with no lesions identified.\n",
            "\n",
            "PREDICTED CONCEPTS:\n",
            "['centrally positioned patent', 'segment al thickening', 'prominent luminal narrowing', 'mosaic attenuation pattern', 'likely small airway disease', 're tic ulo nod ular density', 'para sep tal emphysema', 'several nonspecific nodules', 'no mass', 'no active infiltration', 'contour size', 'normal heart contour', 'no pericardial effusion', 'no thickening', 'calc ific a the roma', 'calc ific plaques', 'caliber', 'normal thoracic esophagus', 'no significant wall thickening', 'no enlarged lymph nodes', 'normal bone structures', 'heights', 'normal upper abdominal organs', 'no liver lesions', 'normal bilateral adrenal', 'normal adrenal glands', 'no lesions']\n",
            "\n",
            "GROUND TRUTH:\n",
            "['patent trachea', 'patent bronchi', 'peribronchial thickening', 'luminal narrowing', 'mosaic attenuation pattern likely small airway disease', 'reticulonodular density', 'paraseptal emphysema', 'subcentimeter nonspecific pulmonary nodules', 'normal heart', 'normal mediastinal main vascular structures', 'no pericardial effusion', 'calcific atheroma plaques', 'normal esophagus', 'no enlarged lymph nodes', 'normal bone structures', 'normal upper abdominal organs', 'no liver lesions', 'normal bilateral adrenal glands']\n",
            "================================================================================\n",
            "REPORT:\n",
            "The trachea and the lumens of both main bronchi are patent. No occlusive pathology is detected in the trachea and the lumens of both main bronchi. Regression of consolidation areas in the middle lobe and lower lobes of the right lung compared to the previous examination. Atelectatic changes are observed in the posterobasal segment of the left lower lobe. Fibroatelectatic changes are present in the anterolateral basal segment and inferior lingular segment of the left lower lobe. No new infiltration areas are detected in the current examination. No bilateral pleural thickening or effusion is noted. Calibration of the thoracic main vascular structures is normal. No dilatation is detected in the thoracic aorta. The heart contour and size are normal. No pericardial thickening or effusion is observed. Mediastinal structures are evaluated as suboptimal due to the unenhanced examination. Thoracic esophagus is normal in caliber with no significant pathological wall thickening. A soft tissue density, possibly remnant thymus tissue, is observed in the anterior mediastinum in a triangular fashion. No lytic or destructive lesions are detected in the bone structures. Upper abdominal sections within the field of view appear normal. An accessory spleen is observed in the splenic hilum. The bilateral adrenal glands are normal in caliber.\n",
            "\n",
            "PREDICTED CONCEPTS:\n",
            "['no oc clusive pathology', 'consolidation areas middle lobe', 'at elect atic changes', 'fibro at elect atic changes', 'no infiltration areas', 'no bilateral pleural thickening', 'no bilateral effusion', 'calibration', 'normal thoracic main vascular structures', 'no dilatation', 'contour size', 'normal heart contour', 'no pericardial thickening', 'no effusion', 'normal thoracic esophagus', 'no pathological wall thickening', 'likely soft tissue density', 'no lytic or lesions are', 'spleen is']\n",
            "\n",
            "GROUND TRUTH:\n",
            "['patent trachea', 'patent bronchi', 'no occlusive pathology in trachea and bronchi', 'consolidation', 'atelectatic changes', 'fibroatelectatic changes', 'no new infiltration areas', 'no pleural thickening', 'no pleural effusion', 'normal thoracic main vascular structures', 'normal thoracic aorta', 'normal heart contour and size', 'no pericardial thickening', 'no pericardial effusion', 'normal esophagus', 'remnant thymus tissue', 'normal bone structures', 'normal upper abdominal sections', 'accessory spleen', 'normal bilateral adrenal glands']\n",
            "================================================================================\n",
            "REPORT:\n",
            "The trachea and both main bronchi are normal. No occlusive pathology is detected in the trachea and both main bronchi. Nodular lesions with ground glass opacity are observed in the upper and middle lobes of the right lung. These findings are nonspecific, but considering clinical context (e.g., pneumonia), they may be due to a specific infection (e.g., fungal). Further evaluation with clinical and laboratory correlation is recommended. Additionally, there are a few subcentimeter nodules in both lungs. No masses are detected in either lung. Minimal emphysematous changes are present in both lungs. Heart contour and size are normal. No pleural or pericardial effusion is detected. The widths of the mediastinal main vascular structures are normal. A central venous catheter is seen on the right, terminating at the superior vena cava-right atrium junction. Mediastinal structures cannot be optimally evaluated due to the absence of contrast material. No pathologically enlarged lymph nodes are observed in the mediastinum or hilar regions. No pathological wall thickening is observed in the esophagus within the sections. No fractures or lytic-destructive lesions are detected in the bone structures within the sections. No upper abdominal free fluid or collections are detected in the sections. No pathologically enlarged lymph nodes are observed.\n",
            "\n",
            "PREDICTED CONCEPTS:\n",
            "['normal trachea', 'normal both main bronchi', 'no oc clusive pathology', 'no d ular lesions', 'no d ular ground glass opacity', 'pneumonia', 'infection', 'fungal', 'nodules lungs', 'no masses', 'minimal emphysema', 'minimal tous changes', 'contour size', 'normal heart contour', 'no pleural', 'no pericardial effusion', 'widths', 'normal media stin al main vascular structures', 'central venous catheter', 'no pathological ly enlarged', 'no pathological wall thickening', 'no fractures', 'no lytic- destructive lesions are', 'no abdominal free fluid or', 'no abdominal free are', 'no ly lymph nodes are']\n",
            "\n",
            "GROUND TRUTH:\n",
            "['normal trachea', 'normal bronchi', 'nodular lesions with ground glass opacity', 'subcentimeter nodules', 'minimal emphysematous changes', 'normal heart contour and size', 'no pleural effusion', 'no pericardial effusion', 'normal mediastinal vascular structures', 'central venous catheter', 'no enlarged lymph nodes', 'normal esophagus', 'no fractures or lytic-destructive bone lesions', 'no upper abdominal free fluid', 'no pathologically enlarged lymph nodes']\n",
            "================================================================================\n",
            "REPORT:\n",
            "In a patient with a history of COVID-19 pneumonia, there is significant progression of parenchymal findings in both lungs, accompanied by widespread linear atelectasis. Other findings are stable.\n",
            "\n",
            "PREDICTED CONCEPTS:\n",
            "['significant parenchyma l findings', 'widespread linear at elect asis', 'other']\n",
            "\n",
            "GROUND TRUTH:\n",
            "['COVID-19 pneumonia', 'progression of parenchymal findings', 'widespread linear atelectasis']\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity2(a: str, b: str) -> float:\n",
        "    a_tokens = set(a.lower().split())\n",
        "    b_tokens = set(b.lower().split())\n",
        "\n",
        "    if not a_tokens and not b_tokens:\n",
        "        return 1.0\n",
        "    if not a_tokens or not b_tokens:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = a_tokens & b_tokens\n",
        "    union = a_tokens | b_tokens\n",
        "\n",
        "    return (2*len(intersection)) / (len(a_tokens)+len(b_tokens))\n"
      ],
      "metadata": {
        "id": "737sN75Bd-wN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fuzzy_counts_one(gt_concepts, pred_concepts, threshold):\n",
        "    gt = [g.strip().lower() for g in gt_concepts if isinstance(g, str) and g.strip()]\n",
        "    pr = [p.strip().lower() for p in pred_concepts if isinstance(p, str) and p.strip()]\n",
        "\n",
        "    used_gt = set()\n",
        "    TP = 0\n",
        "\n",
        "    for p in pr:\n",
        "        best_j = None\n",
        "        best_score = 0.0\n",
        "\n",
        "        for j, g in enumerate(gt):\n",
        "            if j in used_gt:\n",
        "                continue\n",
        "\n",
        "            score = similarity2(p, g)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_j = j\n",
        "\n",
        "        if best_j is not None and best_score >= threshold:\n",
        "            TP += 1\n",
        "            used_gt.add(best_j)\n",
        "\n",
        "    FP = len(pr) - TP\n",
        "    FN = len(gt) - TP\n",
        "\n",
        "    return TP, FP, FN\n"
      ],
      "metadata": {
        "id": "TjNk0EGFeBqk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fuzzy_prf(gt_norm, pred_norm, threshold):\n",
        "    TP = FP = FN = 0\n",
        "\n",
        "    for gt, pr in zip(gt_norm, pred_norm):\n",
        "        t, f, n = fuzzy_counts_one(gt, pr, threshold)\n",
        "        TP += t\n",
        "        FP += f\n",
        "        FN += n\n",
        "\n",
        "    precision = TP / (TP + FP) if TP + FP else 0\n",
        "    recall    = TP / (TP + FN) if TP + FN else 0\n",
        "    f1        = (2 * precision * recall / (precision + recall)) if precision + recall else 0\n",
        "\n",
        "    return precision, recall, f1, (TP, FP, FN)\n"
      ],
      "metadata": {
        "id": "xTNCDX-6eDWr"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for th in [0.7,0.75, 0.8, 0.85,0.9]:\n",
        "    P, R, F1, counts = fuzzy_prf(df[\"gt_concepts\"].tolist(),df[\"pred_concepts\"].tolist(), threshold=th)\n",
        "    print(th, P, R, F1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POWo1liXeFIL",
        "outputId": "be377cf8-c4f2-4a7c-ce5c-456027cb7bd2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7 0.3333333333333333 0.4517374517374517 0.3836065573770492\n",
            "0.75 0.32193732193732194 0.4362934362934363 0.3704918032786885\n",
            "0.8 0.2934472934472934 0.39768339768339767 0.33770491803278685\n",
            "0.85 0.20512820512820512 0.277992277992278 0.2360655737704918\n",
            "0.9 0.17094017094017094 0.23166023166023167 0.19672131147540983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x-6MnSLkeGy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22877200",
        "outputId": "d0986c6a-365e-4b8b-a1bd-90e594af68aa"
      },
      "source": [
        "# df = pd.read_csv(\"/content/option_a.csv\")\n",
        "# df[\"ner_output_parsed\"] = df[\"ner_output\"].apply(ast.literal_eval)\n",
        "# df[\"pred_concepts\"] = df[\"ner_output_parsed\"].apply(postprocess)\n",
        "# def parse_gt(x):\n",
        "#     if isinstance(x, list):\n",
        "#         return x\n",
        "#     if pd.isna(x):\n",
        "#         return []\n",
        "#     try:\n",
        "#         return ast.literal_eval(x)\n",
        "#     except Exception:\n",
        "#         return [str(x)]\n",
        "# df[\"gt_concepts\"] = df[\"concepts\"].apply(parse_gt)\n",
        "\n",
        "# print(\"Re-evaluating F1 scores with updated concept extraction logic:\")\n",
        "# for th in [0.7,0.75, 0.8, 0.85,0.9]:\n",
        "#     P, R, F1, counts = fuzzy_prf(df[\"gt_concepts\"].tolist(),df[\"pred_concepts\"].tolist(), threshold=th)\n",
        "#     print(f\"Threshold: {th}, Precision: {P:.4f}, Recall: {R:.4f}, F1: {F1:.4f}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Re-evaluating F1 scores with updated concept extraction logic:\n",
            "Threshold: 0.7, Precision: 0.3333, Recall: 0.4517, F1: 0.3836\n",
            "Threshold: 0.75, Precision: 0.3219, Recall: 0.4363, F1: 0.3705\n",
            "Threshold: 0.8, Precision: 0.2934, Recall: 0.3977, F1: 0.3377\n",
            "Threshold: 0.85, Precision: 0.2051, Recall: 0.2780, F1: 0.2361\n",
            "Threshold: 0.9, Precision: 0.1709, Recall: 0.2317, F1: 0.1967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLZJUFahhacM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}